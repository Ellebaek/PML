---
title: "Course Project Practical Machine Learning"
author: "Thomas Elleb√¶k"
date: "Friday, July 24, 2015"
output: html_document
---

This is the course project related to the Coursera course "Practical Machine Learning" offered by Jeff Leek, PhD, Roger D. Peng, PhD, Brian Caffo, PhD, from the Johns Hopkins Bloomberg School of Public Health.

## Data
The data for this project comes from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data is delivered by groupware (http://groupware.les.inf.puc-rio.br/har).

## Objective
The goal of this project is to predict the manner in which the participants did the exercise, and explain how the model for the prediction was build.

## Model selection
The response we want to predict is named "classe", and is a multinomial factor variable with the possible labels A, B, C, D and E. So we are looking for a multiclass classifier. Looking at the options introduced in this course, a tree-based model seems obvious. We will therefore look into training and validating a CART model and a Random Forest model.

The data has been split into three parts, one for model training (60%), one for cross-validation (20%) and one for end validation (20%, only used to estimate the out of sample error).

```{r, eval=FALSE}
# get rid of first column with row id
dataTrain <- subset(dataTrain, select=-c(X))

# split data
set.seed(1212)
inTrain <- createDataPartition(dataTrain$classe, p = 0.8)[[1]]
training <- dataTrain[ inTrain,]
endValidation <- dataTrain[-inTrain,]

inCrossVal <- createDataPartition(training$classe, p = 0.25)[[1]]
crossValidation <- training[inCrossVal,]
training <- training[-inCrossVal,]
```

The dimensions of the data set (19622 x 160) makes it complicated to train and validate the chosen models within any reasonable time. So various actions has been taken in order to bring down run-time, in order to be able to finish this exercise before the deadline. First, parallel computing has been activated in R. Second, dimension reduction has been performed together with small sample cross validation.

### Dimension reduction
The original data set contain 160 variables. Exploring the data, we find that many variables are either mainly empty or mainly NAs. Removing the columns with more than 90% empty or more than 90% NAs, reduces the dimensions dramatically.

```{r, eval=FALSE}
percentNA <- colSums(is.na(training))/nrow(training)
colsNotNA <- percentNA < 0.90
percentEmpty <- colSums(training=="", na.rm = TRUE)/nrow(training)
colsNotEmpty <- percentEmpty < 0.90
cols <- colsNotNA & colsNotEmpty

training <- training[cols]
crossValidation <- crossValidation[cols]
endValidation <- endValidation[cols]
```

Still we have more than 50 features to explain the classe-response, so a further reduction is necessary. One approach would be to find highly correlated features and simply remove some of them, but a solution that captures more of the variation in the original data is PCA. We will use PCA although we know that it will reduce interpretability. First we remove a few (obvious!) unimportant non-numeric variables, then we apply the preProcess method from the caret package with method "pca".

```{r, eval=FALSE}
numeric_cols <- sapply(training, is.numeric)

preProc <- preProcess(training[numeric_cols], method="pca", thresh=0.60)
trainPC <- predict(preProc, training[numeric_cols])
```

The threshold was first set to 0.80, meaning that we want the principal components to be able to explain 80% of the variation in the data. Using small sub-samples of the training data (kinda cross validation) in an iterative training of cart and random forest models, and looking at the final model and the variable importance's, the threshold of the PCA was reduced to 0.60, which completed the dimension reduction.

```{r, eval=FALSE}
# cart
modFit <- train(training$classe ~ ., method="rpart", data=trainPC)
print(modFit$finalModel)
varImp(modFit)

# random forest
modFitRF <- train(training$classe ~ ., data=trainPC, method="rf", prox=TRUE)
varImp(modFitRF)
```

### Final Model and out-of-sample error
For each trained model in above iterative PCA feature selection approach, cross validation was used to estimate the out-of-sample accuracy estimates was calculated via the confusionMatrix method. 

```{r, eval=FALSE}
crossValPC <- predict(preProc, crossValidation[numeric_cols])
confusionMatrix(crossValidation$classe, predict(modFitRF, crossValPC))$overall[1]
```

Using cross validation to select features and reduce dimension, and at the same time measure accuracy, gave away that the more advanced random forest method, where a further step of cross validation is performed during the internal bootstrapping of trees (random sampling of variables) and majority voting, is a hard competitor for the more simple CART model. Therefore, Random Forest was chosen as prediction method and the model has been trained with the 60% training set and the out-of-sample error of the model has been estimated via the 20% end validation data set as 1-accuracy=1-0.922=0.078. Due to the selected model, this measure is based on cross validation. Had we chosen the CART model, further cross validation should have been performed in order to estimate the out-of-sample error, for example k-fold.

```{r, eval=FALSE}
endValPC <- predict(preProc, endValidation[numeric_cols])
confusionMatrix(endValidation$classe, predict(modFitRF, endValPC))$overall[1]
```

## Prediction on test cases
20 different test cases has been supplied to test or evaluate our final model. The code for carrying out the test is following.

```{r, eval=FALSE}
dataEvaluationTest <- read.csv("pml-testing.csv", header = TRUE, na.strings="NA")
dataEvaluationTest <- subset(dataEvaluationTest, select=-c(X))
evalTest <- dataEvaluationTest[cols]
evaluationPC <- predict(preProc, evalTest[numeric_cols])
answers <- as.character(predict(modFitRF, evaluationPC))
```

Submission of the predicted response (answers) revealed that the final model correctly predicted 19 out of 20 test cases, giving us a error rate of 0.05, very close to the expected error rate 0.078.
